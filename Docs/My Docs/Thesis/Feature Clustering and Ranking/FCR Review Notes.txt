FCR weaknesses:
1) Dendrogram and manual threshold selection
2) Ranking of clusters is suboptimal 
3) Linear correlation similarity measure is limited 
4) 

Improvents
1) Use AP with MI (?) - we need a better discrete MI with just 10 bins though.  Distances are only calc once and only between ind objects
2) a) Branch and bound on exemplar features
   b) Forward Sel or JMI type on exemplar features (JMI was tested already and we know it did bad overall, so rather use forward / backward selection)
   c) LASSO on exemplars from all clusters (this is like a simiplified version of MultiviewFS that assumes the clustering adequately captures redundancy and that a lin feature-label dependency is a good idea)
3) See https://www.mathworks.com/matlabcentral/fileexchange/30998-kernel-estimate-for--conditional--mutual-information    
   
   
Feature comparison to do
-------------------------
x - Data bootstraps must be identical for each method - with parfor, this may mean not using gendat
  also note that bootstraps should be sampling with replacement whereas gendat([], 0.5) is not sampling with replacement 
x - consider tryinh naivebc with >10 bins or with a density estimator    
   
Review 2 To Do
----------------
x - Compare AP with single pref param to hierarchical clustering 
- Compare hist MI to kernel MI for cluster ranking 
- Do AP exemplar features work better than naivebc etc ranked features
- Compare correlation to distance correlation (to kernelmi)
- Consider other cluster rankings like performance on exemplar or distance correlation with class labs 
- Dataset 2 should maybe only get 4 features 
- Is min accuracy feature more stable than the exemplar 
- Sometimes the the preferred features are not in the top ranked clusters.  Would a FS type formulation help this.  Or possibly branch and bound on exemplars.  
x - Can we speed things up for some fS algorithms by specifying the num features??  I think I didn't specify because something was needed for FsStabilityEval but this shouldn't be necessary anymore...

- Include another fs method in the comparison 

Notes 
------------------
x - resFci gen with RenumClustAcrossBootstraps is done in CompareFsMethods for orig results. For latest code RenumClustAcrossBootstraps is done in BootstrapFsEval.  It is not done the same and this needs understanding.  OK, RenumClustAcrossBootstraps does what it says and does not change FeatIdx and stability measures.  There is falsed out code in BootstrapFsEval and CompareFsMethods, that assigned the renumbered clust idx to feat idx.  This is what (should) improves the stability measures.
- BootstrapFsEval in an older MSC version possibly uses same data for subData1 and subData2 - but the latest version does not... YES - the orig uses same data for both sets 
- I get close to the orig (CompareFsMethodsHs3) accuracies when I use BootstrapFsEval with subData1==subData2.  The stabilities for the first 3 data sets are also close, the last three with >100 features less so.  The bootstraps are obviously/apparently different.  That together with >100 features I think explains the stability anomaly.
x - The stabilities in resFci in CompareFsMethodsHs3.mat are better than the resFci generated by the code segment in CompareFsMethods.  This is dubious and needs understanding...
x - The stabilities in resFci in CompareFsMethodsHs4.mat (which is where the actual paper results come from) do not have this anomaly i.e. resFci gen in CompareFsMethods = resFci from CompareFsMethodsHs4.mat
x - resFci from CompareFsMethodsHs4.mat has a FeatIdx_ field, which comes from CompareFsMethodsHs4.mat
x - Digging into CompareFsMethodsHs4.mat, I see res.FsMapping which contains version which tells you the date (of training) = 6 Oct 2016, and prtools version: 5.3.1.  This matches the current prtools, but does not match the code date I have been comparing against! 
- NB remember to set innerMethods
- FCR-ap-mi-nopref has best stability&accuracy of any FCR option tried so far (with clust acc = acc(exemplar) and bestFeat = exemplar).  Naivebc25 is not necessarily better than naivebc.  FCR-ap-naivebc-nopref is also good.
- clust acc = min acc and clust acc = acc(exemplar) are similar / identical 
- best feat = min acc and best feat = exemplat are similar / identical 
- using mi instead of correlation for clustering makes things worse 
- distcorr works ok as a feature importance measure but mi is better and faster 


Notes
------
- Min accuracy vs exemplar vs median accuracy to rank features from each cluster:
  - for the first 3 datasets, min accuracy is more stable (we get consistency of 1 with feature not cluster indices), 
  for the last 3 datasets, it is not more stable
- Exemplar feature 
x - RenumClustAcrossBootstraps is done in BootstrapFsEval - the stuff in CompareFsMethods should not really be necessary - but this changes things, so what is going on?
x - Naivebc with more bins improves things across all data seemingly - not always
~ - One should NOT call RenumClustAcrossBootstraps > 1x
- NB Something is very wrong with FeatIdx (and therefore stability measures) of featself-mi in the orig results: CompareFsMethodsHs4.mat.  Latest results do not share this problem 


Some results
---------------
1) (with clust acc = acc(exemplar) and bestFeat = exemplar for ap version) 
    'Method'                     'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'                   [   0.7894]    [  0.8703]    [    1.6088]    [ 0.6859]
    'FCR-H-naivebc-nopref'       [   0.4967]    [  0.8665]    [   18.6990]    [ 0.4357]
    'FCR-ap-naivebc'             [   0.6847]    [  0.8766]    [   18.0368]    [ 0.5982]
    'FCR-ap-naivebc25'           [   0.6550]    [  0.8726]    [   18.2097]    [ 0.5716]
    'FCR-ap-naivebc25-nopref'    [   0.4580]    [  0.8750]    [   17.7451]    [ 0.4050]	
(featidx = clustidx)
    'Method'                     'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'                   [   0.9448]    [  0.8703]    [    1.6088]    [ 0.8242]
    'FCR-H-naivebc-nopref'       [   0.8759]    [  0.8665]    [   18.6990]    [ 0.7627]
    'FCR-ap-naivebc'             [   0.9432]    [  0.8766]    [   18.0368]    [ 0.8287]
    'FCR-ap-naivebc25'           [   0.9374]    [  0.8726]    [   18.2097]    [ 0.8199]
    'FCR-ap-naivebc25-nopref'    [   0.9838]    [  0.8750]    [   17.7451]    [ 0.8616]
2) (with clust acc = acc(exemplar) and bestFeat = exemplar for ap version) 
    'Method'                     'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'                   [   0.7894]    [  0.8703]    [    1.6042]    [ 0.6859]
    'FCR-ap-naivebc25-nopref'    [   0.4275]    [  0.8770]    [   18.5773]    [ 0.3777]
    'FCR-ap-mi'                  [   0.7622]    [  0.8771]    [    0.7470]    [ 0.6668]
    'FCR-ap-mi-nopref'           [   0.6301]    [  0.8812]    [    0.7081]    [ 0.5551]
(featidx = clustidx)
    'Method'                     'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'                   [   0.9448]    [  0.8703]    [    1.6042]    [ 0.8242]
    'FCR-ap-naivebc25-nopref'    [   0.9601]    [  0.8770]    [   18.5773]    [ 0.8435]
    'FCR-ap-mi'                  [   0.9772]    [  0.8771]    [    0.7470]    [ 0.8579]
    'FCR-ap-mi-nopref'           [   0.9838]    [  0.8812]    [    0.7081]    [ 0.8675]
3) (with clust acc = acc(exemplar) and bestFeat = exemplar for ap version) 
    'Method'                  'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'                [   0.7894]    [  0.8703]    [    1.6437]    [ 0.6859]
    'FCR-ap-mi-nopref'        [   0.5977]    [  0.8798]    [    0.7321]    [ 0.5255]
    'FCR-ap-mi-jmi-nopref'    [   0.6478]    [  0.8795]    [    0.7711]    [ 0.5703]
(featidx = clustidx)
    'Method'                  'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'                [   0.9448]    [  0.8703]    [    1.6437]    [ 0.8242]
    'FCR-ap-mi-nopref'        [        1]    [  0.8798]    [    0.7321]    [ 0.8798]
    'FCR-ap-mi-jmi-nopref'    [   0.9487]    [  0.8795]    [    0.7711]    [ 0.8364]
4) 	(with clust acc = min acc and bestFeat = exemplar for ap version) 
    'Method'              'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'            [   0.7894]    [  0.8703]    [    1.6153]    [ 0.6859]
    'FCR-ap-mi-nopref'    [   0.5977]    [  0.8798]    [    0.7494]    [ 0.5255]
(featidx = clustidx)
    'Method'              'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'            [   0.9448]    [  0.8703]    [    1.6153]    [ 0.8242]
    'FCR-ap-mi-nopref'    [        1]    [  0.8798]    [    0.7494]    [ 0.8798]
5) 	(with clust acc = min acc and bestFeat = exemplar for ap version) 
    'Method'              'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'            [   0.7894]    [  0.8703]    [    1.6329]    [ 0.6859]
    'FCR-ap-mi-nopref'    [   0.5977]    [  0.8798]    [    0.7523]    [ 0.5255]
(featidx = clustidx)
    'Method'              'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'            [   0.9448]    [  0.8703]    [    1.6329]    [ 0.8242]
    'FCR-ap-mi-nopref'    [        1]    [  0.8798]    [    0.7523]    [ 0.8798]
6)   (with clust acc = min acc and bestFeat = exemplar for ap version and fsFormulation) 
    'Method'              'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'            [   0.7894]    [  0.8703]    [    1.5186]    [ 0.6859]
    'FCR-ap-mi-nopref'    [   0.5459]    [  0.8585]    [    1.2627]    [ 0.4727]
(featidx = clustidx)
    'Method'              'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'            [   0.9448]    [  0.8703]    [    1.5186]    [ 0.8242]
    'FCR-ap-mi-nopref-fsform'    [   0.8228]    [  0.8585]    [    1.2627]    [ 0.7144]	
7)   (with clust acc = exemplar and bestFeat = exemplar for ap version and nmi diss. measure) 
    'Method'                'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'              [   0.7894]    [  0.8703]    [    1.6503]    [ 0.6859]
    'FCR-apmi-mi-nopref'    [   0.5497]    [  0.8513]    [   20.6473]    [ 0.4757]
(featidx = clustidx)
    'Method'                'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-H-mi'              [   0.9448]    [  0.8703]    [    1.6503]    [ 0.8242]
    'FCR-apmi-mi-nopref'    [   0.8016]    [  0.8513]    [   20.6473]    [ 0.6878]
7)   (with clust acc = exemplar and bestFeat = exemplar for ap version and distcorr feat importance measure) 
    'Method'                    'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-ap-distcorr-nopref'    [   0.7545]    [  0.8538]    [   51.5610]    [ 0.6559]
(featidx = clustidx)
    'Method'                    'Stability'    'Accuracy'    'FsDuration'    'Overall'
    'FCR-ap-distcorr-nopref'    [   0.9567]    [  0.8538]    [   51.5610]    [ 0.8211]
	
-----------------------------------------------------------------------------------------------------------------------	
Letter to editor:   

Dear Prof. Warner

I would like to submit a revision of the manuscript entitled “Feature Clustering and Ranking for Selecting Stable Features from High Dimensional Remotely Sensed Data” for possible publication in the International Journal of Remote Sensing.  While I realise that the original manuscript was officially rejected, I believe we have addressed the referees' concerns, as I describe below.  I hope that you will reconsider this submission.  

In response to concerns about the novelty of the manuscript, I would like to highlight the following:
1) To the best of my knowledge, the problem of instability that occurs when selecting features from data containing redundancy has not been explicity addressed or quantified in a remote sensing context.  Our method aims to remove feature redundancy, and therefore the resultant instability, through the clustering step.  We demonstrate the success of this on remote sensing data, using a quantitative measure of stability.  
2) While related methods exist, the feature clustering and ranking (FCR) method (which combines hierarchical clustering for redundancy isolation, and naive Bayes or mutual information criteria for identifying relevance) has not been previously published, to the best of our knowledge.  The combination of techniques used in FCR has some specific advantages: 
   1) Hierarchical clustering is deterministic, fast and does not require prior knowledge of the number of clusters.  2) The relevance criteria do not make assumptions of linear dependence between features and class labels, as is the case with structured sparsity approaches.  
3) Our method is distinct in that it allows factors such as computation time and measurement cost to be considered, in addition to relevance, when selecting features from correlated clusters.
4) There are very few studies that address feature selection in the context of remote sensing in general.  

I believe we have addressed the referees' comments around recently proposed feature selection approaches, structured sparisty regularisation, and the Chen et al (2017) paper, "Supervised Multiview Feature Selection Exploring Homogeneity and Heterogeneity With l1,2-Norm and Automatic View Generation".  Specifically, we have differentiated our method from the general approach of structured sparisty regularisation, and the  Multiview Feature Selection (MultiviewFS) technique of Chen et el (2017).  In summary: 
1) Structured sparsity regularisation approaches minimise an objective function that assumes a linear dependence between features and class labels.  FCR uses a ranking heuristic to select features that does not make assumptions of linearity.
2) MultiviewFS performs redundant feature clustering using a Euclidean distance measure while FCR uses correlation.  Euclidean distance describes feature similarity or proximity, while correlation is a broader description of redundancy that captures general linear dependence.  
Other points raised by the referees have also been accommodated to in the revised manuscript.

I hope that this revision 

The attention to and quantification of stability has not been addressed in any remote sensing journal that I have seen.  The combination of hierarchical clustering and naive Bayes criterion has also never been published, although feature selection methods with other forms of clustering and importance measure have been published.  It also has an element of novelty in that it compares traditional methods on remote sensing data sets which has only been done in a few papers that I have seen and not with the methods or data in my paper.  Admittedly, it is far from revolutionary, but don't these things make it novel enough?

