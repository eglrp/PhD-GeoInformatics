FCR weaknesses:
1) Dendrogram and manual threshold selection
2) Ranking of clusters is suboptimal 
3) Linear correlation similarity measure is limited 
4) 

Improvents
1) Use AP with MI (?) - we need a better discrete MI with just 10 bins though.  Distances are only calc once and only between ind objects
2) a) Branch and bound on exemplar features
   b) Forward Sel or JMI type on exemplar features (JMI was tested already and we know it did bad overall, so rather use forward / backward selection)
   c) LASSO on exemplars from all clusters (this is like a simiplified version of MultiviewFS that assumes the clustering adequately captures redundancy and that a lin feature-label dependency is a good idea)
3) See https://www.mathworks.com/matlabcentral/fileexchange/30998-kernel-estimate-for--conditional--mutual-information    
   
   
Feature comparison to do
-------------------------
- Data bootstraps must be identical for each method - with parfor, this may mean not using gendat
  also note that bootstraps should be sampling with replacement whereas gendat([], 0.5) is not sampling with replacement 
- consider tryinh naivebc with >10 bins or with a density estimator    
   
   
Review 2 To Do
----------------
- Compare AP with single pref param to hierarchical clustering 
- Compare hist MI to kernel MI for cluster ranking 
- Do AP exemplar features work better than naivebc etc ranked features
- Compare correlation to distance correlation (to kernelmi)
- Consider other cluster rankings like performance on exemplar or distance correlation with class labs 
- Dataset 2 should maybe only get 4 features 
- Is min accuracy feature more stable than the exemplar 


Notes
------
- Min accuracy vs exemplar vs median accuracy to rank features from each cluster:
  - for the first 3 datasets, min accuracy is more stable (we get consistency of 1 with feature not cluster indices), 
  for the last 3 datasets, it is not more stable
- Exemplar feature 
- RenumClustAcrossBootstraps is done in BootstrapFsEval - the stuff in CompareFsMethods should not really be necessary - but this changes things, so what is going on?
- Naivebc with more bins improves things across all data seemingly
   
Letter to editor:   

Dear Prof. Warner

I would like to submit a revision of the manuscript entitled “Feature Clustering and Ranking for Selecting Stable Features from High Dimensional Remotely Sensed Data” for possible publication in the International Journal of Remote Sensing.  While I realise that the original manuscript was officially rejected, I believe we have addressed the referees' concerns, as I describe below.  I hope that you will reconsider this submission.  

In response to concerns about the novelty of the manuscript, I would like to highlight the following:
1) To the best of my knowledge, the problem of instability that occurs when selecting features from data containing redundancy has not been explicity addressed or quantified in a remote sensing context.  Our method aims to remove feature redundancy, and therefore the resultant instability, through the clustering step.  We demonstrate the success of this on remote sensing data, using a quantitative measure of stability.  
2) While related methods exist, the feature clustering and ranking (FCR) method (which combines hierarchical clustering for redundancy isolation, and naive Bayes or mutual information criteria for identifying relevance) has not been previously published, to the best of our knowledge.  The combination of techniques used in FCR has some specific advantages: 
   1) Hierarchical clustering is deterministic, fast and does not require prior knowledge of the number of clusters.  2) The relevance criteria do not make assumptions of linear dependence between features and class labels, as is the case with structured sparsity approaches.  
3) Our method is distinct in that it allows factors such as computation time and measurement cost to be considered, in addition to relevance, when selecting features from correlated clusters.
4) There are very few studies that address feature selection in the context of remote sensing in general.  

I believe we have addressed the referees' comments around recently proposed feature selection approaches, structured sparisty regularisation, and the Chen et al (2017) paper, "Supervised Multiview Feature Selection Exploring Homogeneity and Heterogeneity With l1,2-Norm and Automatic View Generation".  Specifically, we have differentiated our method from the general approach of structured sparisty regularisation, and the  Multiview Feature Selection (MultiviewFS) technique of Chen et el (2017).  In summary: 
1) Structured sparsity regularisation approaches minimise an objective function that assumes a linear dependence between features and class labels.  FCR uses a ranking heuristic to select features that does not make assumptions of linearity.
2) MultiviewFS performs redundant feature clustering using a Euclidean distance measure while FCR uses correlation.  Euclidean distance describes feature similarity or proximity, while correlation is a broader description of redundancy that captures general linear dependence.  
Other points raised by the referees have also been accommodated to in the revised manuscript.

I hope that this revision 

The attention to and quantification of stability has not been addressed in any remote sensing journal that I have seen.  The combination of hierarchical clustering and naive Bayes criterion has also never been published, although feature selection methods with other forms of clustering and importance measure have been published.  It also has an element of novelty in that it compares traditional methods on remote sensing data sets which has only been done in a few papers that I have seen and not with the methods or data in my paper.  Admittedly, it is far from revolutionary, but don't these things make it novel enough?

